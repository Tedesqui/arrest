<!DOCTYPE html> <html lang="pt-BR"> <head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title>Reconhecimento Facial AR</title> <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script> <style> body, html { margin: 0; overflow: hidden; font-family: Arial, sans-serif; background: black; color: white; } video, canvas { position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; object-fit: cover; transform: scaleX(-1); z-index: 0; } #info { position: fixed; bottom: 20px; left: 50%; transform: translateX(-50%); background: rgba(0,0,0,0.7); color: white; padding: 15px; border-radius: 10px; font-size: 1rem; text-align: center; z-index: 2; max-width: 90%; white-space: pre-line; } </style> </head> <body>
<video id="video" autoplay muted playsinline></video>
<canvas id="canvas"></canvas>

<div id="info">Carregando IA e modelos...</div> <script> const video = document.getElementById('video'); const canvas = document.getElementById('canvas'); const info = document.getElementById('info'); const people = [ { name: "Joao", info: "Professor de Matemática, 35 anos", images: 7 }, { name: "Maria", info: "Arquiteta, 29 anos", images: 3 }, { name: "Carlos", info: "Engenheiro, 41 anos", images: 3 } ]; async function loadLabeledImages() { const labeledDescriptors = await Promise.all( people.map(async person => { const descriptors = []; for (let i = 1; i <= person.images; i++) { try { const img = await faceapi.fetchImage(`labeled/${person.name}/${i}.jpg`); const detection = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor(); if (detection) descriptors.push(detection.descriptor); } catch (e) { console.warn(`Erro carregando imagem de ${person.name}:`, e); } } return new faceapi.LabeledFaceDescriptors(person.name, descriptors); }) ); return labeledDescriptors; } async function startCamera() { const stream = await navigator.mediaDevices.getUserMedia({ video: true }); video.srcObject = stream; await new Promise(resolve => video.onloadedmetadata = resolve); } async function init() { info.innerText = 'Carregando modelos de IA...'; await Promise.all([ faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights'), faceapi.nets.faceLandmark68Net.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights'), faceapi.nets.faceRecognitionNet.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights') ]); await startCamera(); const labeledDescriptors = await loadLabeledImages(); const matcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6); canvas.width = video.videoWidth; canvas.height = video.videoHeight; const ctx = canvas.getContext('2d'); info.innerText = 'Pronto! Aproxime um rosto da câmera.'; setInterval(async () => { const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()) .withFaceLandmarks() .withFaceDescriptors(); const resizedDetections = faceapi.resizeResults(detections, { width: canvas.width, height: canvas.height }); ctx.clearRect(0, 0, canvas.width, canvas.height); faceapi.draw.drawDetections(canvas, resizedDetections); if (detections.length > 0) { const foundPeople = resizedDetections.map(det => { const best = matcher.findBestMatch(det.descriptor); const match = people.find(p => p.name === best.label); return match ? `${match.name} - ${match.info}` : 'Pessoa não reconhecida'; }); info.innerText = foundPeople.join('\n\n'); } else { info.innerText = 'Nenhum rosto detectado.'; } }, 1000); } window.addEventListener('load', init); </script> </body> </html>
