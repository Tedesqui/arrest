<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reconhecimento Facial AR com Zoom - Câmera Traseira</title>

    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

    <style>
        body, html {
            margin: 0;
            overflow: hidden; /* Evita barras de rolagem */
            font-family: Arial, sans-serif;
            background: black;
            color: white;
            display: flex;
            flex-direction: column; /* Para posicionar os controles */
            align-items: center;
            justify-content: center;
            min-height: 100vh;
        }

        /* Estilo para o elemento de vídeo (feed da câmera) */
        video {
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw; /* Ocupa 100% da largura da viewport */
            height: 100vh; /* Ocupa 100% da altura da viewport */
            object-fit: cover; /* Cobre toda a área sem distorcer */
            /* REMOVIDO: transform: scaleX(-1); pois a câmera traseira geralmente não precisa ser espelhada. */
            z-index: 0; /* Fica no fundo */
        }

        /* Estilo para o canvas de detecção (gerado pela face-api.js) */
        canvas {
            position: fixed;
            top: 0;
            left: 0;
            z-index: 1; /* Fica acima do vídeo */
            pointer-events: none; /* Permite cliques "atravessarem" o canvas */
        }

        /* Estilo para a caixa de informações/status */
        #info {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%); /* Centraliza horizontalmente */
            background: rgba(0, 0, 0, 0.7); /* Fundo semi-transparente */
            color: white;
            padding: 15px;
            border-radius: 10px;
            font-size: 1.1rem; /* Um pouco maior para melhor leitura */
            text-align: center;
            z-index: 2; /* Fica acima do canvas */
            max-width: 90%; /* Limita a largura para melhor leitura em telas grandes */
            white-space: pre-line; /* Preserva quebras de linha no texto */
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3); /* Sombra para destaque */
        }

        /* Estilo para o contêiner de controles (botões) */
        #controls {
            position: fixed;
            top: 10px;
            right: 10px;
            z-index: 3; /* Fica acima de todos os outros elementos */
        }

        /* Estilo para o botão de zoom */
        button {
            font-size: 18px;
            padding: 10px 18px;
            background: #007bff; /* Cor azul padrão */
            color: white;
            border: none;
            border-radius: 30px; /* Botão mais arredondado */
            cursor: pointer;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2); /* Sombra para o botão */
            transition: background-color 0.2s ease, transform 0.1s ease;
        }

        button:hover {
            background-color: #0056b3; /* Azul mais escuro ao passar o mouse */
            transform: translateY(-2px); /* Efeito de "levantar" */
        }

        button:active {
            transform: translateY(0); /* Retorna à posição normal ao clicar */
        }

        button:disabled {
            background-color: #6c757d; /* Cinza quando desabilitado */
            cursor: not-allowed;
            opacity: 0.7;
            box-shadow: none;
        }
    </style>
</head>
<body>

    <video id="video" autoplay muted playsinline></video>

    <canvas id="drawingCanvas"></canvas>

    <div id="info">Carregando IA e modelos...</div>

    <div id="controls">
        <button id="zoomToggleBtn" disabled>Zoom</button>
    </div>

    <script>
        // --- 1. Seletores de Elementos HTML ---
        const video = document.getElementById("video");
        const drawingCanvas = document.getElementById("drawingCanvas");
        const info = document.getElementById("info");
        const zoomToggleBtn = document.getElementById('zoomToggleBtn');

        // --- 2. Dados de Pessoas para Reconhecimento ---
        // Array de objetos, cada um contendo o nome, informações adicionais
        // e o número de imagens de treinamento para essa pessoa.
        // As imagens devem estar na pasta 'labeled/<nome_da_pessoa>/<numero>.jpg'
        const people = [
            { name: "Joao", info: "Professor de Matemática, 35 anos", images: 3 },
            { name: "Maria", info: "Arquiteta, 29 anos", images: 3 },
            { name: "Carlos", info: "Engenheiro, 41 anos", images: 3 }
        ];

        // --- 3. Variáveis de Controle da Câmera e Zoom ---
        let mediaTrack; // Armazena a MediaStreamTrack da câmera para controle de zoom
        let zoomCapabilities = {}; // Armazena as capacidades de zoom (min, max, step)
        let currentZoom = 1.0; // Nível de zoom atual da câmera
        let zoomDirection = 1; // 1: aumentar zoom, -1: redefinir/diminuir zoom
        // Fator de incremento/decremento se 'step' não for definido. Usado para calcular o 'passo' do zoom.
        const ZOOM_INCREMENT_FACTOR = 0.5;

        // --- 4. Funções Assíncronas ---

        /**
         * Carrega as imagens rotuladas de cada pessoa e extrai seus descritores faciais.
         * Esses descritores são usados para treinar o modelo de reconhecimento.
         * @returns {Promise<faceapi.LabeledFaceDescriptors[]>} Uma promessa que resolve para um array de descritores faciais rotulados.
         */
        async function loadLabeledImages() {
            updateInfoStatus("Carregando imagens de treinamento...");
            const labeledDescriptors = await Promise.all(
                people.map(async person => {
                    const descriptors = [];
                    for (let i = 1; i <= person.images; i++) {
                        const imgPath = `labeled/${person.name}/${i}.jpg`;
                        try {
                            const img = await faceapi.fetchImage(imgPath);
                            // Detecta o rosto, pontos de referência e descritores na imagem
                            const detection = await faceapi.detectSingleFace(img)
                                .withFaceLandmarks()
                                .withFaceDescriptor();
                            if (detection) {
                                descriptors.push(detection.descriptor); // Adiciona o descritor se houver detecção
                            }
                        } catch (err) {
                            console.warn(`Erro ao carregar ou processar imagem ${imgPath} para ${person.name}:`, err);
                        }
                    }
                    if (descriptors.length === 0) {
                        console.warn(`Nenhum descritor encontrado para ${person.name}. Verifique se as imagens estão corretas em 'labeled/${person.name}/'.`);
                    }
                    return new faceapi.LabeledFaceDescriptors(person.name, descriptors);
                })
            );
            // Verifica se há pelo menos um descritor válido para alguma pessoa
            if (labeledDescriptors.every(ld => ld.descriptors.length === 0)) {
                throw new Error("Nenhuma pessoa reconhecível configurada. Verifique as imagens em 'labeled/'.");
            }
            return labeledDescriptors;
        }

        /**
         * Inicia o stream de vídeo da CÂMERA TRASEIRA do usuário e configura as capacidades de zoom.
         */
        async function startCamera() {
            try {
                // Solicita acesso à câmera, especificando 'facingMode: environment' para a câmera traseira
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        facingMode: { exact: "environment" } // ESSA LINHA FORÇA A CÂMERA TRASEIRA
                    }
                });
                video.srcObject = stream; // Define o stream como fonte do elemento <video>

                // Espera o vídeo carregar metadados para ter as dimensões corretas e a trilha de vídeo
                await new Promise(resolve => video.onloadedmetadata = resolve);

                // Obtém a trilha de vídeo para controlar o zoom
                mediaTrack = stream.getVideoTracks()[0];

                // Verifica e configura as capacidades de zoom
                if (mediaTrack && typeof mediaTrack.getCapabilities === 'function') {
                    zoomCapabilities = mediaTrack.getCapabilities();
                    if (zoomCapabilities.zoom) { // Verifica se a câmera suporta zoom
                        const { min, current } = zoomCapabilities.zoom;
                        currentZoom = current || min; // Define o zoom inicial como o atual ou o mínimo
                        zoomToggleBtn.disabled = false; // Habilita o botão de zoom
                        console.log('Capacidades de Zoom:', zoomCapabilities.zoom);
                        updateInfoStatus(`Câmera traseira iniciada. Zoom: ${currentZoom.toFixed(2)}x`);
                    } else {
                        updateInfoStatus('Câmera traseira iniciada. Zoom não suportado neste dispositivo.');
                        zoomToggleBtn.disabled = true; // Desabilita o botão se não houver suporte
                    }
                } else {
                    updateInfoStatus('Câmera traseira iniciada. Navegador não suporta controle de zoom.');
                    zoomToggleBtn.disabled = true; // Desabilita o botão se a API não estiver disponível
                }

            } catch (err) {
                console.error("Erro ao acessar a câmera traseira:", err);
                updateInfoStatus(`Erro: Câmera traseira não acessível. ${err.message}. Verifique as permissões.`);
                zoomToggleBtn.disabled = true;
            }
        }

        /**
         * Aplica o nível de zoom na câmera.
         * @param {number} targetZoom O valor de zoom desejado.
         */
        async function applyZoom(targetZoom) {
            if (!mediaTrack || !zoomCapabilities.zoom) {
                updateInfoStatus("Zoom não suportado ou câmera não iniciada.");
                return;
            }

            const { min, max, step } = zoomCapabilities.zoom;
            // Garante que o zoom esteja dentro dos limites min/max
            let newZoom = Math.max(min, Math.min(max, targetZoom));

            // Arredonda para o passo mais próximo, se 'step' for definido
            if (step && step > 0) {
                newZoom = Math.round(newZoom / step) * step;
            }

            try {
                await mediaTrack.applyConstraints({
                    advanced: [{ zoom: newZoom }]
                });
                currentZoom = newZoom; // Atualiza o zoom atual
                updateInfoStatus(`Zoom: ${currentZoom.toFixed(2)}x`);

                // Atualiza o texto do botão de zoom para indicar a próxima ação
                if (currentZoom >= max) {
                    zoomToggleBtn.textContent = "Redefinir Zoom";
                } else if (currentZoom <= min) {
                    zoomToggleBtn.textContent = "Zoom +";
                } else {
                    zoomToggleBtn.textContent = "Zoom +/-"; // Se estiver entre min e max
                }

            } catch (err) {
                console.error('Erro ao aplicar zoom:', err);
                updateInfoStatus(`Erro ao aplicar zoom: ${err.message}`);
            }
        }

        /**
         * Alterna o nível de zoom da câmera (aumenta ou redefine para o mínimo).
         */
        async function toggleZoom() {
            if (!mediaTrack || !zoomCapabilities.zoom) {
                updateInfoStatus("Zoom não suportado pela câmera ou navegador.");
                return;
            }

            const { min, max, step } = zoomCapabilities.zoom;
            let nextZoom;
            // Define o incremento: usa 'step' da câmera ou calcula um baseado no fator
            const increment = step || (max - min) * ZOOM_INCREMENT_FACTOR;

            if (zoomDirection === 1) { // Aumentar o zoom
                nextZoom = currentZoom + increment;
                if (nextZoom >= max) {
                    nextZoom = max; // Garante que não exceda o máximo
                    zoomDirection = -1; // Altera para diminuir/redefinir no próximo clique
                }
            } else { // Diminuir ou redefinir o zoom
                nextZoom = min; // Volta para o zoom mínimo
                zoomDirection = 1; // Altera para aumentar no próximo clique
            }

            await applyZoom(nextZoom); // Aplica o novo nível de zoom
        }

        /**
         * Atualiza a área de informações/status na parte inferior da tela.
         * @param {string} message A mensagem a ser exibida.
         */
        function updateInfoStatus(message) {
            info.innerText = message;
        }

        /**
         * Inicializa a aplicação:
         * 1. Carrega os modelos de IA da Face-API.js.
         * 2. Inicia o feed da câmera e configura o zoom.
         * 3. Carrega e processa as imagens de treinamento para reconhecimento.
         * 4. Configura o loop principal de detecção e reconhecimento facial.
         */
        async function initApp() {
            updateInfoStatus("Carregando modelos de IA...");
            try {
                // Carrega todos os modelos de redes neurais necessários da face-api.js em paralelo.
                // Verifica se a biblioteca faceapi está carregada antes de usar.
                if (typeof faceapi === "undefined") {
                    throw new Error("A biblioteca face-api.js não foi carregada. Verifique o CDN ou a conexão.");
                }
                await Promise.all([
                    faceapi.nets.tinyFaceDetector.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights"),
                    faceapi.nets.faceLandmark68Net.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights"),
                    faceapi.nets.faceRecognitionNet.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights")
                ]);
            } catch (err) {
                console.error("Erro ao carregar modelos da face-api.js:", err);
                updateInfoStatus(`Erro: Falha ao carregar modelos de IA. ${err.message}.`);
                return; // Impede que o resto da app inicie se os modelos não carregarem.
            }

            await startCamera(); // Inicia o feed de vídeo da câmera traseira e configura o zoom.

            let labeledDescriptors;
            try {
                labeledDescriptors = await loadLabeledImages();
            } catch (err) {
                console.error("Erro ao carregar imagens rotuladas:", err);
                updateInfoStatus(`Erro: ${err.message}`);
                return;
            }

            // Cria o "FaceMatcher" para comparar rostos detectados com os conhecidos.
            const matcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6); // 0.6 é o limiar de distância para correspondência.

            updateInfoStatus(`Pronto! Aproxime um rosto da câmera. Zoom: ${currentZoom.toFixed(2)}x`);

            // --- 5. Event Listener para o Vídeo e Loop Principal de Detecção ---
            // Adiciona um listener para o evento 'play' do vídeo, garantindo que o vídeo esteja pronto.
            video.addEventListener("play", () => {
                // Configura o canvas para desenhar as detecções para ter as mesmas dimensões do vídeo.
                drawingCanvas.width = video.videoWidth;
                drawingCanvas.height = video.videoHeight;
                const ctx = drawingCanvas.getContext("2d");

                // Inicia um loop periódico para detecção e reconhecimento facial.
                setInterval(async () => {
                    // Detecta todos os rostos no frame atual do vídeo, extraindo pontos de referência
                    // e descritores faciais usando um detector otimizado (TinyFaceDetectorOptions).
                    const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                        .withFaceLandmarks()
                        .withFaceDescriptors();

                    // Redimensiona os resultados da detecção para o tamanho do canvas.
                    const resizedDetections = faceapi.resizeResults(detections, { width: drawingCanvas.width, height: drawingCanvas.height });

                    // Limpa o canvas e desenha as caixas delimitadoras dos rostos detectados.
                    ctx.clearRect(0, 0, drawingCanvas.width, drawingCanvas.height);
                    faceapi.draw.drawDetections(drawingCanvas, resizedDetections);

                    if (detections.length > 0) {
                        let foundPeopleInfo = [];
                        // Para cada rosto detectado, encontra a melhor correspondência no FaceMatcher.
                        resizedDetections.forEach(det => {
                            const bestMatch = matcher.findBestMatch(det.descriptor);
                            // Busca as informações completas da pessoa no array 'people'.
                            const person = people.find(p => p.name === bestMatch.label);

                            if (person) {
                                foundPeopleInfo.push(`${person.name} - ${person.info}`); // Adiciona informações da pessoa reconhecida.
                            } else {
                                foundPeopleInfo.push("Pessoa não reconhecida"); // Mensagem para rostos não identificados.
                            }
                        });
                        // Exibe as informações das pessoas reconhecidas na sobreposição.
                        updateInfoStatus(foundPeopleInfo.join('\n\n'));
                    } else {
                        // Se nenhum rosto for detectado, atualiza a mensagem.
                        updateInfoStatus("Nenhum rosto detectado.");
                    }
                }, 1000); // Intervalo de 1 segundo para detecção.
            });
        }

        // --- 6. Event Listeners ---
        // Adiciona o event listener para o botão de zoom.
        zoomToggleBtn.addEventListener('click', toggleZoom);

        // --- 7. Inicialização da Aplicação ---
        // A função 'initApp' é chamada apenas quando todo o conteúdo da janela (incluindo scripts) estiver carregado.
        window.addEventListener('load', initApp);
    </script>
</body>
</html>
